{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3cf8bf",
   "metadata": {},
   "source": [
    "# **五、生成任务**\n",
    "\n",
    "任务定义： 文本生成是NLP中较为复杂的任务，本章只聚焦于文本到文本的生成，例如生成式摘要、机器翻译、对话生成等等。本文以其中的对话生成和机器翻译为切入点，介绍其中的经典方法。\n",
    "\n",
    "## 5.1 对话生成 \n",
    "\n",
    "### 5.1.1 任务简介\n",
    "\n",
    "基于生成的方式将对话生成问题看作是一种“源到目标”的映射问题，直接从大量的训练数据中学习从输入信息到最终输出之间的映射关系。   \n",
    "\n",
    "\n",
    "\n",
    "### 5.1.2 方法：bert-base-s2s、gpt\n",
    "\n",
    "代码案例：https://zhuanlan.zhihu.com/p/170358507\n",
    "\n",
    "\n",
    "\n",
    "## 5.2 机器翻译 \n",
    "\n",
    "### 5.2.1 机器翻译原理\n",
    "\n",
    "机器翻译就是将一个语言的句子翻译成另一个语言的句子，主要可以分为三个步骤：**「预处理、翻译模型、后处理」**。\n",
    "\n",
    "预处理是对源语言的句子进行规范化处理，把过长的句子通过标点符号分成几个短句子，过滤一些语气词和与意思无关的文字，将一些数字和表达不规范的地方，归整成符合规范的句子，等等。\n",
    "\n",
    "翻译模块是将输入的字符单元、序列翻译成目标语言序列的过程，这是机器翻译中最关键最核心的地方。纵观机器翻译发展的历史，翻译模块可以分为基于规则的翻译、基于统计的翻译和基于神经网络的翻译三大类。现如今基于神经网络的机器翻译已经成为了主流方法，效果也远远超过了前两类方法。\n",
    "\n",
    "后处理模块是将翻译结果进行大小写的转化、建模单元进行拼接，特殊符号进行处理，使得翻译结果更加符合人们的阅读习惯。\n",
    "\n",
    "\n",
    "\n",
    "### 5.2.2 任务简介\n",
    "\n",
    "机器翻译和神经网络有着千丝万缕的关系。神经网络是一种方法，而机器翻译是其中最大的目标应用场景之一，很多神经网络技术的发展，最早就是从做机器翻译任务开始的。\n",
    "\n",
    "机器翻译任务从机器学习的角度看，是一种生成任务，因为它要输出的结果不是类别编号，而是一串字符序列。在神经网络和深度学习中，做机器翻译任务的模型，也称为Seq2Seq(sequence to sequence）模型。\n",
    "\n",
    "Seq2Seq模型是一种经典的深度学习模型，通常采用Encoder-Decoder框架。这个框架最早出自2014年的一篇论文《Learning Phrase Representations using RNN Encoder-Decoder forStatistical Machine Translation》，没错，这又是一篇研究机器翻译的经典论文。在这篇论文的基础上，谷歌最终于2016年推出了基于神经网络的机器翻译并大获成功，而这篇论文所提出来的Encoder-Decoder框架，甚至超越了机器翻译领域。\n",
    "\n",
    "做NLP的同学应该都很熟悉Encoder-Decoder框架，现在做CV的同学也开始在熟悉这个框架。最近深度学习领域有一条重磅消息，在NLP领域称霸多年的Transformer模型，现在正在CV领域大杀特杀。NLP和CV一直是机器学习最热门的两个研究领域，不过长期以来一直有点生殖隔离的意思，现在让Transformer一拳打穿了次元壁，所以好几位大牛都在预测机器学习的大一统模型也许正在呼之欲出。\n",
    "\n",
    "不过，Transformer最早是用来做什么的？没错，还是做机器翻译。2017年5月，知名的深度学习研究团队FAIR搞出了个新模型，没给起名字，总之是用CNN+Attention来做机器翻译。论文一出，圈内哗然，毕竟在大家的认知中，CNN模型一向是用来处理图像，也就是做CV的，做文本做机器翻译这块，当时主要还是用RNN及其派生的LSTM等模型。RNN有个很大的缺点，就是没法并行训练，非常耗时所以用起来很鸡肋，现在FAIR用并行性好得多的CNN搞出了新模型，一下有种众望所归的感觉。结果另一家知名的研究团队Google Brain不干了，直接走力大飞砖的路线出了一篇爆款论文，叫《AttentionIs All You Need》，相信大家都有所耳闻。Google Brain没有明说，不过我总觉得这个霸气侧漏的标题显然多少有点暗指FAIR画蛇添足的意思：机器翻译还要什么CNN？直接Attention就完事了。在这篇论文里诞生了一款模型，现在我们都耳熟能详了，这就是Transformer。\n",
    "\n",
    "\n",
    "\n",
    "### 5.3.3 方法：rnn-base-s2s、bert-base-s2s\n",
    "\n",
    "\n",
    "\n",
    "### 5.2.4 发展脉络\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
